{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ca01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6 - Warmup 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': range(1, 21),\n",
    "    'age': np.random.randint(18, 70, 20),\n",
    "    'revenue': np.random.uniform(200, 2000, 20).round(2),\n",
    "    'country': np.random.choice(['US', 'CA', 'UK', 'AU'], 20),\n",
    "    'is_active': np.random.choice([True, False], 20)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['high_value'] = (df['revenue'] > 1000) & df['is_active'] \n",
    "\n",
    "df['target_segment'] = ((df['age'] < 30) | (df['age'] > 60)) & df['country'].isin(['US', 'CA'])\n",
    "\n",
    "df.loc[df['high_value'] | (df['target_segment'] & (df['revenue'] > 500))]\n",
    "\n",
    "# Rewrite the above using .query instead\n",
    "filtered_df = df.query('high_value == True or (target_segment == True and revenue > 500)')\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2084609",
   "metadata": {},
   "source": [
    "# Grade: A-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21238523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup 2 datetime index manipulation \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dates = pd.date_range('2025-01-01', periods=30, freq='D')\n",
    "df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': np.random.randint(100, 500, 30),\n",
    "    'visits': np.random.randint(50, 200, 30)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('date')\n",
    "\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "df['week_of_year'] = df.index.isocalendar().week\n",
    "\n",
    "df['is_weekend'] = df['day_of_week'] >= 5\n",
    "\n",
    "\n",
    "\n",
    "# I've never tried to do a centered window before so this is just a regular one\n",
    "\n",
    "df['sales_7d_avg'] = df.rolling('7d', min_periods= 7, center=True)['sales'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c5061",
   "metadata": {},
   "source": [
    "# Grade: A-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b03e5dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup 3 - Broadcasting and Normalisation \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': ['U1', 'U2', 'U3', 'U4', 'U5'],\n",
    "    'math_score': [85, 92, 78, 88, 95],\n",
    "    'reading_score': [80, 85, 90, 82, 88],\n",
    "    'science_score': [88, 91, 85, 90, 92]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d6ff4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>math_score</th>\n",
       "      <th>reading_score</th>\n",
       "      <th>science_score</th>\n",
       "      <th>total_score</th>\n",
       "      <th>math_pct</th>\n",
       "      <th>reading_pct</th>\n",
       "      <th>science_pct</th>\n",
       "      <th>math_zscore</th>\n",
       "      <th>reading_zscore</th>\n",
       "      <th>science_zscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>253</td>\n",
       "      <td>0.335968</td>\n",
       "      <td>0.316206</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>-0.395120</td>\n",
       "      <td>-1.212678</td>\n",
       "      <td>-0.432450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>91</td>\n",
       "      <td>268</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.317164</td>\n",
       "      <td>0.339552</td>\n",
       "      <td>0.668665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U3</td>\n",
       "      <td>78</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>253</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.355731</td>\n",
       "      <td>0.335968</td>\n",
       "      <td>-1.458906</td>\n",
       "      <td>1.212678</td>\n",
       "      <td>-1.513575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U4</td>\n",
       "      <td>88</td>\n",
       "      <td>82</td>\n",
       "      <td>90</td>\n",
       "      <td>260</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.315385</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.060788</td>\n",
       "      <td>-0.727607</td>\n",
       "      <td>0.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U5</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>275</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.334545</td>\n",
       "      <td>1.124573</td>\n",
       "      <td>0.727607</td>\n",
       "      <td>1.009050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  math_score  reading_score  science_score  total_score  math_pct  \\\n",
       "0      U1          85             80             88          253  0.335968   \n",
       "1      U2          92             85             91          268  0.343284   \n",
       "2      U3          78             90             85          253  0.308300   \n",
       "3      U4          88             82             90          260  0.338462   \n",
       "4      U5          95             88             92          275  0.345455   \n",
       "\n",
       "   reading_pct  science_pct  math_zscore  reading_zscore  science_zscore  \n",
       "0     0.316206     0.347826    -0.395120       -1.212678       -0.432450  \n",
       "1     0.317164     0.339552     0.668665        0.000000        0.648675  \n",
       "2     0.355731     0.335968    -1.458906        1.212678       -1.513575  \n",
       "3     0.315385     0.346154     0.060788       -0.727607        0.288300  \n",
       "4     0.320000     0.334545     1.124573        0.727607        1.009050  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_score'] = df[['math_score', 'reading_score', 'science_score']].sum(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "df['math_pct'] = df['math_score']/df['total_score']\n",
    "\n",
    "df['reading_pct'] = df['reading_score']/df['total_score']\n",
    "\n",
    "df['science_pct'] = df['science_score']/df['total_score']\n",
    "\n",
    "\n",
    "\n",
    "df['math_zscore'] = (df['math_score'] - df['math_score'].mean())/df['math_score'].std()\n",
    "\n",
    "df['reading_zscore'] = (df['reading_score'] - df['reading_score'].mean())/df['reading_score'].std()\n",
    "\n",
    "df['science_zscore'] = (df['science_score'] - df['science_score'].mean())/df['science_score'].std()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5ecd4",
   "metadata": {},
   "source": [
    "# Grade: A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "843c0cbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['math_score', 'reading_score', 'science_score'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m score_cols = [\u001b[33m'\u001b[39m\u001b[33mmath_score\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mreading_score\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscience_score\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m pct_cols = [\u001b[33m'\u001b[39m\u001b[33mmath_pct\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mreading_pct\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mscience_pct\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df[pct_cols] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscore_cols\u001b[49m\u001b[43m]\u001b[49m.div(df[\u001b[33m'\u001b[39m\u001b[33mtotal_score\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_analytics_practice/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_analytics_practice/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/python_analytics_practice/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['math_score', 'reading_score', 'science_score'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Potential optimisations\n",
    "score_cols = ['math_score', 'reading_score', 'science_score']\n",
    "pct_cols = ['math_pct', 'reading_pct', 'science_pct']\n",
    "\n",
    "df[pct_cols] = df[score_cols].div(df['total_score'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99924b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential optimisations\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df[['math_zscore', 'reading_zscore', 'science_zscore']] = df[score_cols].apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ce2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup 4 - Strings and Regex \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': ['U001', 'U002', 'U003', 'U004', 'U005'],\n",
    "    'email': [\n",
    "        'john.doe@company.com',\n",
    "        'jane_smith@gmail.com',\n",
    "        'invalid-email',\n",
    "        'bob.jones@company.co.uk',\n",
    "        'alice@subdomain.example.org'\n",
    "    ],\n",
    "    'phone': [\n",
    "        '555-1234',\n",
    "        '555.5678',\n",
    "        '5559012',\n",
    "        '555-3456',\n",
    "        '(555) 7890'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2892b9b",
   "metadata": {},
   "source": [
    "Create a column email_domain that extracts the domain from the email (everything after the @). If there's no @, set it to None.\n",
    "Create a column is_valid_email that is True if the email contains:\n",
    "\n",
    "- At least one character before @\n",
    "- An @ symbol\n",
    "- At least one . after the @\n",
    "\n",
    "Otherwise False.\n",
    "\n",
    "Create a column phone_clean that extracts only the digits from the phone column (remove all non-digit characters).\n",
    "Create a column phone_formatted that formats phone_clean as XXX-XXXX (assumes all phones are 7 digits after cleaning). If phone_clean is not exactly 7 digits, set to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d913d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U001</td>\n",
       "      <td>john.doe@company.com</td>\n",
       "      <td>555-1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U002</td>\n",
       "      <td>jane_smith@gmail.com</td>\n",
       "      <td>555.5678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U003</td>\n",
       "      <td>invalid-email</td>\n",
       "      <td>5559012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U004</td>\n",
       "      <td>bob.jones@company.co.uk</td>\n",
       "      <td>555-3456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U005</td>\n",
       "      <td>alice@subdomain.example.org</td>\n",
       "      <td>(555) 7890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                        email       phone\n",
       "0    U001         john.doe@company.com    555-1234\n",
       "1    U002         jane_smith@gmail.com    555.5678\n",
       "2    U003                invalid-email     5559012\n",
       "3    U004      bob.jones@company.co.uk    555-3456\n",
       "4    U005  alice@subdomain.example.org  (555) 7890"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass\n",
    "df['email_domain'] = df[\"email\"].str.extract(r\".*\\@(.+)\")\n",
    "\n",
    "df['is_valid_email'] = df['email'].str.contains(r\".+\\@.+\")\n",
    "\n",
    "df['phone_clean'] = df['phone'].str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "df['phone_formatted'] = df['phone_clean'].str.replace(r'(^\\d{4})(\\d{3}$)', r'\\1-\\2', regex=True)\n",
    "\n",
    "df.loc[~df['phone_formatted'].str.match(r'^\\d{4}-\\d{3}$'), 'phone_formatted'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89372a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>email_domain</th>\n",
       "      <th>is_valid_email</th>\n",
       "      <th>phone_clean</th>\n",
       "      <th>phone_formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U001</td>\n",
       "      <td>john.doe@company.com</td>\n",
       "      <td>555-1234</td>\n",
       "      <td>company.com</td>\n",
       "      <td>True</td>\n",
       "      <td>5551234</td>\n",
       "      <td>5551-234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U002</td>\n",
       "      <td>jane_smith@gmail.com</td>\n",
       "      <td>555.5678</td>\n",
       "      <td>gmail.com</td>\n",
       "      <td>True</td>\n",
       "      <td>5555678</td>\n",
       "      <td>5555-678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U003</td>\n",
       "      <td>invalid-email</td>\n",
       "      <td>5559012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>5559012</td>\n",
       "      <td>5559-012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U004</td>\n",
       "      <td>bob.jones@company.co.uk</td>\n",
       "      <td>555-3456</td>\n",
       "      <td>company.co.uk</td>\n",
       "      <td>True</td>\n",
       "      <td>5553456</td>\n",
       "      <td>5553-456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U005</td>\n",
       "      <td>alice@subdomain.example.org</td>\n",
       "      <td>(555) 7890</td>\n",
       "      <td>subdomain.example.org</td>\n",
       "      <td>True</td>\n",
       "      <td>5557890</td>\n",
       "      <td>5557-890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                        email       phone           email_domain  \\\n",
       "0    U001         john.doe@company.com    555-1234            company.com   \n",
       "1    U002         jane_smith@gmail.com    555.5678              gmail.com   \n",
       "2    U003                invalid-email     5559012                    NaN   \n",
       "3    U004      bob.jones@company.co.uk    555-3456          company.co.uk   \n",
       "4    U005  alice@subdomain.example.org  (555) 7890  subdomain.example.org   \n",
       "\n",
       "   is_valid_email phone_clean phone_formatted  \n",
       "0            True     5551234        5551-234  \n",
       "1            True     5555678        5555-678  \n",
       "2           False     5559012        5559-012  \n",
       "3            True     5553456        5553-456  \n",
       "4            True     5557890        5557-890  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e481bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['email_domain'] = df['email'].str.extract(r'.*@(.+)')\n",
    "\n",
    "# Fixed - search for an actual . after @, which was in the spec. \n",
    "df['is_valid_email'] = df['email'].str.contains(r'.+@.+\\..+')\n",
    "\n",
    "df['phone_clean'] = df['phone'].str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "# Fixes - 3/4 not 4/3 formatting\n",
    "\n",
    "# Option 1: Two-step (your approach, fixed)\n",
    "df['phone_formatted'] = df['phone_clean'].str.replace(r'^(\\d{3})(\\d{4})$', r'\\1-\\2', regex=True)\n",
    "df.loc[~df['phone_formatted'].str.match(r'^\\d{3}-\\d{4}$'), 'phone_formatted'] = None\n",
    "\n",
    "# Option 2: One-step with .extract()\n",
    "df['phone_formatted'] = df['phone_clean'].str.extract(r'^(\\d{3})(\\d{4})$').apply(\n",
    "    lambda x: f'{x[0]}-{x[1]}' if x.notna().all() else None, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a21f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate 1. Join + Aggregate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Users table\n",
    "users = pd.DataFrame({\n",
    "    'user_id': ['U1', 'U2', 'U3', 'U4', 'U5'],\n",
    "    'username': ['alice', 'bob', 'charlie', 'diana', 'eve'],\n",
    "    'signup_date': pd.to_datetime(['2024-01-15', '2024-02-20', '2024-01-10', '2024-03-05', '2024-02-28'])\n",
    "})\n",
    "\n",
    "# Purchases table\n",
    "purchases = pd.DataFrame({\n",
    "    'purchase_id': range(1, 16),\n",
    "    'user_id': ['U1', 'U1', 'U2', 'U1', 'U3', 'U2', 'U4', 'U1', 'U3', 'U2', 'U5', 'U3', 'U4', 'U2', 'U1'],\n",
    "    'category': ['electronics', 'books', 'electronics', 'clothing', 'books', \n",
    "                 'electronics', 'clothing', 'books', 'electronics', 'books',\n",
    "                 'clothing', 'clothing', 'electronics', 'clothing', 'electronics'],\n",
    "    'amount': [299.99, 19.99, 450.00, 89.99, 24.99, 199.99, 120.00, 15.99, \n",
    "               350.00, 29.99, 75.00, 110.00, 399.99, 95.00, 599.99]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f13726",
   "metadata": {},
   "source": [
    "1) For each user, calculate:\n",
    "\n",
    "- total_revenue: sum of all their purchase amounts\n",
    "- n_purchases: count of their purchases\n",
    "- avg_purchase: mean purchase amount\n",
    "\n",
    "\n",
    "2) Rank users by total_revenue (highest = rank 1). Add this as a column revenue_rank.\n",
    "3) Merge this aggregated data back to the users table (left join, so all users appear even if they have no purchases).\n",
    "4) Fill missing values:\n",
    "- total_revenue, n_purchases, avg_purchase → 0\n",
    "- revenue_rank → NaN (leave as-is for users with no purchases)\n",
    "\n",
    "\n",
    "\n",
    "Final output should be the users table with these added columns:\n",
    "- total_revenue\n",
    "- n_purchases\n",
    "- avg_purchase\n",
    "- revenue_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2be508f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>signup_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1</td>\n",
       "      <td>alice</td>\n",
       "      <td>2024-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2</td>\n",
       "      <td>bob</td>\n",
       "      <td>2024-02-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U3</td>\n",
       "      <td>charlie</td>\n",
       "      <td>2024-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U4</td>\n",
       "      <td>diana</td>\n",
       "      <td>2024-03-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U5</td>\n",
       "      <td>eve</td>\n",
       "      <td>2024-02-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id username signup_date\n",
       "0      U1    alice  2024-01-15\n",
       "1      U2      bob  2024-02-20\n",
       "2      U3  charlie  2024-01-10\n",
       "3      U4    diana  2024-03-05\n",
       "4      U5      eve  2024-02-28"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()\n",
    "# purchases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34d5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  user_id username signup_date  total_revenue  n_purchases  avg_purchase  \\\n",
      "0      U1    alice  2024-01-15        1025.95            5    205.190000   \n",
      "1      U2      bob  2024-02-20         774.98            4    193.745000   \n",
      "2      U3  charlie  2024-01-10         484.99            3    161.663333   \n",
      "3      U4    diana  2024-03-05         519.99            2    259.995000   \n",
      "4      U5      eve  2024-02-28          75.00            1     75.000000   \n",
      "\n",
      "   revenue_rank  \n",
      "0           1.0  \n",
      "1           2.0  \n",
      "2           4.0  \n",
      "3           3.0  \n",
      "4           5.0  \n"
     ]
    }
   ],
   "source": [
    "combined = users.merge(purchases, on = 'user_id', how = 'left')\n",
    "\n",
    "\n",
    "\n",
    "combined = combined.groupby('user_id').agg(\n",
    "    total_revenue = (\"amount\", 'sum'), \n",
    "    n_purchases= (\"purchase_id\", 'count'),\n",
    "    avg_purchase= (\"amount\", 'mean')\n",
    ")\n",
    "\n",
    "combined['revenue_rank'] = combined['total_revenue'].rank(method = 'dense', ascending = False)\n",
    "\n",
    "final = users.merge(combined, on = 'user_id', how = 'left')\n",
    "\n",
    "final['total_revenue'] = final['total_revenue'].fillna(0)\n",
    "\n",
    "final['n_purchases'] = final['n_purchases'].fillna(0)\n",
    "\n",
    "final['avg_purchase'] = final['avg_purchase'].fillna(0)\n",
    "\n",
    "\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb3777",
   "metadata": {},
   "source": [
    "# Grade: A-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "665a6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key issue - joining then aggregating doesn't work with count(). For users with no purchases, they'll get a row with NaN, and count() will return 1 instead of 0. \n",
    "# A better strategy is to aggregate first, then join.\n",
    "\n",
    "# Aggregate purchases by user\n",
    "purchase_agg = purchases.groupby('user_id').agg(\n",
    "    total_revenue=('amount', 'sum'),\n",
    "    n_purchases=('purchase_id', 'count'),\n",
    "    avg_purchase=('amount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Rank by revenue\n",
    "purchase_agg['revenue_rank'] = purchase_agg['total_revenue'].rank(method='dense', ascending=False)\n",
    "\n",
    "# Merge back to users\n",
    "final = users.merge(purchase_agg, on='user_id', how='left')\n",
    "\n",
    "# Fill NaNs\n",
    "final['total_revenue'] = final['total_revenue'].fillna(0)\n",
    "final['n_purchases'] = final['n_purchases'].fillna(0)\n",
    "final['avg_purchase'] = final['avg_purchase'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fd28b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6: Intermediate 2 - MultiIndex Time Features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Daily sales data for multiple stores\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-01-01', periods=90, freq='D').tolist() * 3,\n",
    "    'store_id': ['S1']*90 + ['S2']*90 + ['S3']*90,\n",
    "    'daily_sales': np.random.randint(1000, 5000, 270)\n",
    "})\n",
    "\n",
    "df = df.sort_values(['store_id', 'date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ddd8f",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "1.\tFor each store, calculate: \n",
    "- weekly_sales: sum of daily_sales for each week (group by store + week)\n",
    "- monthly_sales: sum of daily_sales for each month (group by store + month)\n",
    "2.\tMerge these weekly and monthly totals back to the daily-level DataFrame, so each daily row shows: \n",
    "- The week's total sales for that store\n",
    "- The month's total sales for that store\n",
    "3.\tCreate a column daily_pct_of_week that shows what percentage of the week's total sales came from that specific day.\n",
    "4.\tCreate a column daily_pct_of_month that shows what percentage of the month's total sales came from that specific day.\n",
    "Final output: Daily-level DataFrame with added columns:\n",
    "- weekly_sales\n",
    "- monthly_sales\n",
    "- daily_pct_of_week\n",
    "- daily_pct_of_month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "288817b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date store_id  daily_sales  weekly_sales  monthly_sales  \\\n",
      "0   2025-01-01       S1         4174           NaN            NaN   \n",
      "1   2025-01-02       S1         4507           NaN            NaN   \n",
      "2   2025-01-03       S1         1860           NaN            NaN   \n",
      "3   2025-01-04       S1         2294           NaN            NaN   \n",
      "4   2025-01-05       S1         2130       14965.0            NaN   \n",
      "..         ...      ...          ...           ...            ...   \n",
      "265 2025-03-27       S3         4445           NaN            NaN   \n",
      "266 2025-03-28       S3         4743           NaN            NaN   \n",
      "267 2025-03-29       S3         2631           NaN            NaN   \n",
      "268 2025-03-30       S3         2495       23873.0            NaN   \n",
      "269 2025-03-31       S3         4304           NaN        95890.0   \n",
      "\n",
      "     daily_pct_of_week  daily_pct_of_month  \n",
      "0                  NaN                 NaN  \n",
      "1                  NaN                 NaN  \n",
      "2                  NaN                 NaN  \n",
      "3                  NaN                 NaN  \n",
      "4            14.233211                 NaN  \n",
      "..                 ...                 ...  \n",
      "265                NaN                 NaN  \n",
      "266                NaN                 NaN  \n",
      "267                NaN                 NaN  \n",
      "268          10.451137                 NaN  \n",
      "269                NaN            4.488476  \n",
      "\n",
      "[270 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70548/1237979520.py:2: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_sales = df.groupby('store_id').resample('M', on = 'date')['daily_sales'].sum()\n"
     ]
    }
   ],
   "source": [
    "weekly_sales = df.groupby('store_id').resample('W', on = 'date')['daily_sales'].sum() \n",
    "monthly_sales = df.groupby('store_id').resample('M', on = 'date')['daily_sales'].sum()\n",
    "\n",
    "df = df.merge(weekly_sales.to_frame(name='weekly_sales'), left_on=['store_id', 'date'], right_index=True, how = 'left') \n",
    "df = df.merge(monthly_sales.to_frame(name='monthly_sales') , left_on=['store_id', 'date'], right_index=True, how = 'left')\n",
    "\n",
    "df['daily_pct_of_week'] = df['daily_sales']/df['weekly_sales'] * 100\n",
    "\n",
    "df['daily_pct_of_month'] = df['daily_sales']/df['monthly_sales'] * 100\n",
    "\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "996f242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date store_id  daily_sales                   week    month  \\\n",
      "0   2025-01-01       S1         4174  2024-12-30/2025-01-05  2025-01   \n",
      "1   2025-01-02       S1         4507  2024-12-30/2025-01-05  2025-01   \n",
      "2   2025-01-03       S1         1860  2024-12-30/2025-01-05  2025-01   \n",
      "3   2025-01-04       S1         2294  2024-12-30/2025-01-05  2025-01   \n",
      "4   2025-01-05       S1         2130  2024-12-30/2025-01-05  2025-01   \n",
      "..         ...      ...          ...                    ...      ...   \n",
      "265 2025-03-27       S3         4445  2025-03-24/2025-03-30  2025-03   \n",
      "266 2025-03-28       S3         4743  2025-03-24/2025-03-30  2025-03   \n",
      "267 2025-03-29       S3         2631  2025-03-24/2025-03-30  2025-03   \n",
      "268 2025-03-30       S3         2495  2025-03-24/2025-03-30  2025-03   \n",
      "269 2025-03-31       S3         4304  2025-03-31/2025-04-06  2025-03   \n",
      "\n",
      "     daily_sales_weekly  daily_sales_monthly  daily_sales_weekly  \\\n",
      "0                 14965                94786               14965   \n",
      "1                 14965                94786               14965   \n",
      "2                 14965                94786               14965   \n",
      "3                 14965                94786               14965   \n",
      "4                 14965                94786               14965   \n",
      "..                  ...                  ...                 ...   \n",
      "265               23873                95890               23873   \n",
      "266               23873                95890               23873   \n",
      "267               23873                95890               23873   \n",
      "268               23873                95890               23873   \n",
      "269                4304                95890                4304   \n",
      "\n",
      "     daily_sales_monthly  \n",
      "0                  94786  \n",
      "1                  94786  \n",
      "2                  94786  \n",
      "3                  94786  \n",
      "4                  94786  \n",
      "..                   ...  \n",
      "265                95890  \n",
      "266                95890  \n",
      "267                95890  \n",
      "268                95890  \n",
      "269                95890  \n",
      "\n",
      "[270 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70548/1189760977.py:9: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_sales = df.groupby('store_id').resample('M', on='date')['daily_sales'].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Create period columns in your original df\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "# Resample and reset index to get period columns\n",
    "weekly_sales = df.groupby('store_id').resample('W', on='date')['daily_sales'].sum().reset_index()\n",
    "weekly_sales['week'] = weekly_sales['date'].dt.to_period('W')\n",
    "\n",
    "monthly_sales = df.groupby('store_id').resample('M', on='date')['daily_sales'].sum().reset_index()\n",
    "monthly_sales['month'] = monthly_sales['date'].dt.to_period('M')\n",
    "\n",
    "# Merge on store + period\n",
    "df = df.merge(weekly_sales[['store_id', 'week', 'daily_sales']], \n",
    "              on=['store_id', 'week'], \n",
    "              suffixes=('', '_weekly'))\n",
    "df = df.merge(monthly_sales[['store_id', 'month', 'daily_sales']], \n",
    "              on=['store_id', 'month'], \n",
    "              suffixes=('', '_monthly'))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18495488",
   "metadata": {},
   "source": [
    "# Grade: B\n",
    "\n",
    "- Didn't need resample after groupby\n",
    "- Didn't rename columns to the spec provided \n",
    "- Didn't use transform which is cleaner here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea3e8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date store_id  daily_sales                   week    month  \\\n",
      "0   2025-01-01       S1         4174  2024-12-30/2025-01-05  2025-01   \n",
      "1   2025-01-02       S1         4507  2024-12-30/2025-01-05  2025-01   \n",
      "2   2025-01-03       S1         1860  2024-12-30/2025-01-05  2025-01   \n",
      "3   2025-01-04       S1         2294  2024-12-30/2025-01-05  2025-01   \n",
      "4   2025-01-05       S1         2130  2024-12-30/2025-01-05  2025-01   \n",
      "..         ...      ...          ...                    ...      ...   \n",
      "265 2025-03-27       S3         4445  2025-03-24/2025-03-30  2025-03   \n",
      "266 2025-03-28       S3         4743  2025-03-24/2025-03-30  2025-03   \n",
      "267 2025-03-29       S3         2631  2025-03-24/2025-03-30  2025-03   \n",
      "268 2025-03-30       S3         2495  2025-03-24/2025-03-30  2025-03   \n",
      "269 2025-03-31       S3         4304  2025-03-31/2025-04-06  2025-03   \n",
      "\n",
      "     weekly_sales  monthly_sales  daily_pct_of_week  daily_pct_of_month  \n",
      "0           14965          94786           0.278917            0.044036  \n",
      "1           14965          94786           0.301169            0.047549  \n",
      "2           14965          94786           0.124290            0.019623  \n",
      "3           14965          94786           0.153291            0.024202  \n",
      "4           14965          94786           0.142332            0.022472  \n",
      "..            ...            ...                ...                 ...  \n",
      "265         23873          95890           0.186194            0.046355  \n",
      "266         23873          95890           0.198676            0.049463  \n",
      "267         23873          95890           0.110208            0.027438  \n",
      "268         23873          95890           0.104511            0.026019  \n",
      "269          4304          95890           1.000000            0.044885  \n",
      "\n",
      "[270 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cleaner pattern: \n",
    "\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "\n",
    "df['weekly_sales'] = df.groupby(['store_id', 'week'])['daily_sales'].transform('sum')\n",
    "df['monthly_sales'] = df.groupby(['store_id', 'month'])['daily_sales'].transform('sum')\n",
    "\n",
    "df['daily_pct_of_week'] = df['daily_sales'] / df['weekly_sales']\n",
    "df['daily_pct_of_month'] = df['daily_sales'] / df['monthly_sales']\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d292d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6 — Intermediate 3: Binning & Categorization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'student_id': range(1, 21),\n",
    "    'exam_score': [45, 67, 89, 72, 91, 58, 77, 83, 65, 94, \n",
    "                   50, 88, 76, 62, 85, 70, 93, 55, 80, 68],\n",
    "    'age': [18, 19, 20, 18, 21, 19, 20, 22, 18, 21,\n",
    "            19, 20, 18, 19, 21, 20, 22, 19, 20, 18]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40523219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    student_id  exam_score  age letter_grade score_tier score_quartile  \\\n",
      "0            1          45   18            F        low             Q1   \n",
      "1            2          67   19            D     medium             Q2   \n",
      "2            3          89   20            B       high             Q4   \n",
      "3            4          72   18            C     medium             Q2   \n",
      "4            5          91   21            A       high             Q4   \n",
      "5            6          58   19            F        low             Q1   \n",
      "6            7          77   20            C     medium             Q3   \n",
      "7            8          83   22            B       high             Q3   \n",
      "8            9          65   18            D     medium             Q2   \n",
      "9           10          94   21            A       high             Q4   \n",
      "10          11          50   19            F        low             Q1   \n",
      "11          12          88   20            B       high             Q4   \n",
      "12          13          76   18            C     medium             Q3   \n",
      "13          14          62   19            D     medium             Q1   \n",
      "14          15          85   21            B       high             Q3   \n",
      "15          16          70   20            C     medium             Q2   \n",
      "16          17          93   22            A       high             Q4   \n",
      "17          18          55   19            F        low             Q1   \n",
      "18          19          80   20            B       high             Q3   \n",
      "19          20          68   18            D     medium             Q2   \n",
      "\n",
      "   age_group  \n",
      "0      18-19  \n",
      "1      18-19  \n",
      "2      20-21  \n",
      "3      18-19  \n",
      "4      20-21  \n",
      "5      18-19  \n",
      "6      20-21  \n",
      "7        22+  \n",
      "8      18-19  \n",
      "9      20-21  \n",
      "10     18-19  \n",
      "11     20-21  \n",
      "12     18-19  \n",
      "13     18-19  \n",
      "14     20-21  \n",
      "15     20-21  \n",
      "16       22+  \n",
      "17     18-19  \n",
      "18     20-21  \n",
      "19     18-19  \n"
     ]
    }
   ],
   "source": [
    "# Default is right = True, which means 'ignore the lowest number and inclusive of the highest number in each bin'. \n",
    "# Setting right = False means 'include the lowest number and ignore the highest number in each bin'. So for example, with right = False, \n",
    "# a score of 60 would be included in the 'D' bin (60-70) rather than the 'F' bin (0-60).\n",
    "\n",
    "df['letter_grade'] = pd.cut(df['exam_score'], bins = [0, 60, 70, 80, 90, 101], labels = ['F', 'D', 'C', 'B', 'A'], include_lowest=True, right = False)\n",
    "\n",
    "df['score_tier'] = pd.cut(df['exam_score'], bins = 3, labels = ['low', 'medium', 'high'], include_lowest=True)\n",
    "df['score_quartile'] = pd.qcut(df['exam_score'], q = 4, labels = ['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "df['age_group'] = pd.cut(df['age'], bins = [18, 20, 22, 100], labels = ['18-19', '20-21', '22+'], right = False)\n",
    "\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1896ec3",
   "metadata": {},
   "source": [
    "# Grade: A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48c40d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6 — Intermediate 4: Lagged Panel Features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': ['U1', 'U1', 'U1', 'U1', 'U1',\n",
    "                'U2', 'U2', 'U2', 'U2', 'U2',\n",
    "                'U3', 'U3', 'U3', 'U3', 'U3'],\n",
    "    'date': pd.to_datetime([\n",
    "        '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-04', '2025-01-05',\n",
    "        '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-04', '2025-01-05',\n",
    "        '2025-01-01', '2025-01-02', '2025-01-03', '2025-01-04', '2025-01-05'\n",
    "    ]),\n",
    "    'revenue': [100, 150, 120, 180, 200,\n",
    "                80, 90, 110, 95, 105,\n",
    "                200, 210, 190, 220, 230]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c44b1f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2025-01-01\n",
       "1    2025-01-02\n",
       "2    2025-01-03\n",
       "3    2025-01-04\n",
       "4    2025-01-05\n",
       "5    2025-01-01\n",
       "6    2025-01-02\n",
       "7    2025-01-03\n",
       "8    2025-01-04\n",
       "9    2025-01-05\n",
       "10   2025-01-01\n",
       "11   2025-01-02\n",
       "12   2025-01-03\n",
       "13   2025-01-04\n",
       "14   2025-01-05\n",
       "Name: date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['user_id', 'date']).reset_index(drop=True)['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c8ab460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id       date  revenue  revenue_lag1  revenue_lag2  \\\n",
      "0       U1 2025-01-01      100           NaN           NaN   \n",
      "1       U1 2025-01-02      150         100.0           NaN   \n",
      "2       U1 2025-01-03      120         150.0         100.0   \n",
      "3       U1 2025-01-04      180         120.0         150.0   \n",
      "4       U1 2025-01-05      200         180.0         120.0   \n",
      "5       U2 2025-01-01       80           NaN           NaN   \n",
      "6       U2 2025-01-02       90          80.0           NaN   \n",
      "7       U2 2025-01-03      110          90.0          80.0   \n",
      "8       U2 2025-01-04       95         110.0          90.0   \n",
      "9       U2 2025-01-05      105          95.0         110.0   \n",
      "10      U3 2025-01-01      200           NaN           NaN   \n",
      "11      U3 2025-01-02      210         200.0           NaN   \n",
      "12      U3 2025-01-03      190         210.0         200.0   \n",
      "13      U3 2025-01-04      220         190.0         210.0   \n",
      "14      U3 2025-01-05      230         220.0         190.0   \n",
      "\n",
      "    revenue_rolling_3d  revenue_change  \n",
      "0           100.000000             NaN  \n",
      "1           125.000000            50.0  \n",
      "2           123.333333           -30.0  \n",
      "3           150.000000            60.0  \n",
      "4           166.666667            20.0  \n",
      "5            80.000000             NaN  \n",
      "6            85.000000            10.0  \n",
      "7            93.333333            20.0  \n",
      "8            98.333333           -15.0  \n",
      "9           103.333333            10.0  \n",
      "10          200.000000             NaN  \n",
      "11          205.000000            10.0  \n",
      "12          200.000000           -20.0  \n",
      "13          206.666667            30.0  \n",
      "14          213.333333            10.0  \n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(['user_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "df['revenue_lag1'] = df.groupby('user_id')['revenue'].shift(1)\n",
    "\n",
    "df['revenue_lag2'] = df.groupby('user_id')['revenue'].shift(2)\n",
    "\n",
    "df['revenue_rolling_3d'] = df.groupby('user_id').rolling('3D', on = 'date')['revenue'].mean().reset_index(drop=True)\n",
    "\n",
    "df['revenue_change'] = df['revenue'] - df['revenue_lag1']\n",
    "\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975fa9d",
   "metadata": {},
   "source": [
    "# Grade: A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28ff8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6 — Intermediate 5: Resample vs Groupby\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'date': pd.date_range('2025-01-01', periods=30, freq='D'),\n",
    "    'category': ['A', 'B'] * 15,\n",
    "    'sales': np.random.randint(100, 500, 30)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7110584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    week category  sales  weekly_pct\n",
      "0  2024-12-30/2025-01-05        A    743    0.531854\n",
      "1  2024-12-30/2025-01-05        B    654    0.468146\n",
      "2  2025-01-06/2025-01-12        A    771    0.437571\n",
      "3  2025-01-06/2025-01-12        B    991    0.562429\n",
      "4  2025-01-13/2025-01-19        A   1569    0.691799\n",
      "5  2025-01-13/2025-01-19        B    699    0.308201\n",
      "6  2025-01-20/2025-01-26        A   1304    0.500576\n",
      "7  2025-01-20/2025-01-26        B   1301    0.499424\n",
      "8  2025-01-27/2025-02-02        A    765    0.626536\n",
      "9  2025-01-27/2025-02-02        B    456    0.373464\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(['date', 'category'])\n",
    "\n",
    "resample_weekly = df.resample('W', on = 'date')['sales'].sum()\n",
    "\n",
    "resample_groupby = df.groupby(df['date'].dt.to_period('W'))['sales'].sum()\n",
    "\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "\n",
    "weekly_categories = df.groupby(['week', 'category'])['sales'].sum().reset_index()\n",
    "\n",
    "weekly_categories['weekly_pct'] = weekly_categories.groupby('week')['sales'].transform(lambda x: x/x.sum())\n",
    "\n",
    "\n",
    "print(weekly_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fcf5e",
   "metadata": {},
   "source": [
    "# Grade: A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67bd9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6 — Hard 1: Funnel Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': ['U1', 'U1', 'U1', 'U1',\n",
    "                'U2', 'U2', 'U2',\n",
    "                'U3', 'U3',\n",
    "                'U4', 'U4', 'U4', 'U4',\n",
    "                'U5', 'U5', 'U5'],\n",
    "    'event_date': pd.to_datetime([\n",
    "        '2025-01-01', '2025-01-01', '2025-01-02', '2025-01-03',\n",
    "        '2025-01-01', '2025-01-02', '2025-01-03',\n",
    "        '2025-01-01', '2025-01-02',\n",
    "        '2025-01-01', '2025-01-01', '2025-01-02', '2025-01-03',\n",
    "        '2025-01-01', '2025-01-02', '2025-01-02'\n",
    "    ]),\n",
    "    'event_type': [\n",
    "        'page_view', 'add_to_cart', 'add_to_cart', 'purchase',\n",
    "        'page_view', 'add_to_cart', 'purchase',\n",
    "        'page_view', 'page_view',\n",
    "        'page_view', 'add_to_cart', 'add_to_cart', 'purchase',\n",
    "        'page_view', 'add_to_cart', 'purchase'\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53495da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['user_id', 'event_date'])\n",
    "\n",
    "\n",
    "\n",
    "# Note - drop_duplicates is not a groupby operation as initially though. Using subset argument gives the expected groupby functionality. \n",
    "\n",
    "\n",
    "\n",
    "event_counts = df.drop_duplicates(subset=['user_id', 'event_type'], keep='first')['event_type'].value_counts()\n",
    "event_counts = event_counts.reset_index(name = 'count').rename(columns = {'index': 'event_type'})\n",
    "\n",
    "page_to_cart = event_counts.loc[event_counts['event_type'] == 'add_to_cart', 'count'].iloc[0] / event_counts.loc[event_counts['event_type'] == 'page_view', 'count'].iloc[0]\n",
    "cart_to_purchase = event_counts.loc[event_counts['event_type'] == 'purchase', 'count'].iloc[0] / event_counts.loc[event_counts['event_type'] == 'add_to_cart', 'count'].iloc[0]\n",
    "page_to_purchase = event_counts.loc[event_counts['event_type'] == 'purchase', 'count'].iloc[0] / event_counts.loc[event_counts['event_type'] == 'page_view', 'count'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "10882334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>event_type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>add_to_cart</th>\n",
       "      <th>page_view</th>\n",
       "      <th>purchase</th>\n",
       "      <th>valid_funnel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2</td>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U3</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U4</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U5</td>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "event_type user_id add_to_cart  page_view   purchase  valid_funnel\n",
       "0               U1  2025-01-01 2025-01-01 2025-01-03         False\n",
       "1               U2  2025-01-02 2025-01-01 2025-01-03          True\n",
       "2               U3         NaT 2025-01-01        NaT         False\n",
       "3               U4  2025-01-01 2025-01-01 2025-01-03         False\n",
       "4               U5  2025-01-02 2025-01-01 2025-01-02         False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  df.drop_duplicates(subset=['user_id', 'event_type'], keep='first')\n",
    "\n",
    "df_wide = df.pivot_table(index = 'user_id', columns = 'event_type', values='event_date').reset_index()\n",
    "df_wide['valid_funnel'] = (df_wide['purchase'] > df_wide['add_to_cart']) & (df_wide['add_to_cart'] > df_wide['page_view'])\n",
    "\n",
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2c4f5269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funnel_stage</th>\n",
       "      <th>count</th>\n",
       "      <th>conversion_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>page_view</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add_to_cart</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>purchase</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  funnel_stage  count  conversion_rate\n",
       "0    page_view      5              NaN\n",
       "1  add_to_cart      4              0.8\n",
       "2     purchase      4              1.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary = pd.DataFrame({\n",
    "    'funnel_stage': ['page_view', 'add_to_cart', 'purchase'],\n",
    "    'count': [df_wide['page_view'].count(), df_wide['add_to_cart'].count(), df_wide['purchase'].count()],\n",
    "    'conversion_rate': [np.nan, page_to_cart, cart_to_purchase]\n",
    "})\n",
    "\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92edc0ab",
   "metadata": {},
   "source": [
    "# Grade: B+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c60dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coach corrected verson. Main difference is using a cleaner way to get counts rather than value_counts and then havnig to use iloc repeatedly. \n",
    "\n",
    "# Get first occurrence of each event per user\n",
    "df_first = df.sort_values(['user_id', 'event_date']).drop_duplicates(subset=['user_id', 'event_type'], keep='first')\n",
    "\n",
    "# Pivot to wide\n",
    "df_wide = df_first.pivot_table(index='user_id', columns='event_type', values='event_date').reset_index()\n",
    "\n",
    "# Valid funnel (all steps present AND in order)\n",
    "df_wide['valid_funnel'] = (\n",
    "    df_wide['page_view'].notna() &\n",
    "    df_wide['add_to_cart'].notna() &\n",
    "    df_wide['purchase'].notna() &\n",
    "    (df_wide['page_view'] < df_wide['add_to_cart']) &\n",
    "    (df_wide['add_to_cart'] < df_wide['purchase'])\n",
    ")\n",
    "\n",
    "# Count users at each step\n",
    "counts = df_wide[['page_view', 'add_to_cart', 'purchase']].notna().sum()\n",
    "\n",
    "# Summary DataFrame\n",
    "df_summary = pd.DataFrame({\n",
    "    'funnel_step': ['page_view', 'add_to_cart', 'purchase'],\n",
    "    'n_users': [counts['page_view'], counts['add_to_cart'], counts['purchase']]\n",
    "})\n",
    "\n",
    "df_summary['conversion_rate'] = df_summary['n_users'] / df_summary['n_users'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c7d5a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 6 — Hard 2: Cohort + Rolling Retention Hybrid\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"user\": [\n",
    "        \"U1\",\"U1\",\"U1\",\"U1\",\n",
    "        \"U2\",\"U2\",\"U2\",\n",
    "        \"U3\",\"U3\",\n",
    "        \"U4\",\"U4\",\"U4\"\n",
    "    ],\n",
    "    \"event_date\": pd.to_datetime([\n",
    "        \"2025-01-01\",\"2025-01-03\",\"2025-01-10\",\"2025-01-20\",\n",
    "        \"2025-01-02\",\"2025-01-08\",\"2025-01-18\",\n",
    "        \"2025-01-05\",\"2025-01-25\",\n",
    "        \"2025-01-07\",\"2025-01-09\",\"2025-01-15\"\n",
    "    ]),\n",
    "    \"event_type\": [\n",
    "        \"visit\",\"visit\",\"purchase\",\"visit\",\n",
    "        \"visit\",\"visit\",\"purchase\",\n",
    "        \"visit\",\"visit\",\n",
    "        \"visit\",\"purchase\",\"visit\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b06d5397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cohort_week</th>\n",
       "      <th>event_week</th>\n",
       "      <th>n_users</th>\n",
       "      <th>retention_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-30/2025-01-05</td>\n",
       "      <td>2024-12-30/2025-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-30/2025-01-05</td>\n",
       "      <td>2025-01-06/2025-01-12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-30/2025-01-05</td>\n",
       "      <td>2025-01-13/2025-01-19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-30/2025-01-05</td>\n",
       "      <td>2025-01-20/2025-01-26</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-06/2025-01-12</td>\n",
       "      <td>2025-01-06/2025-01-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-01-06/2025-01-12</td>\n",
       "      <td>2025-01-13/2025-01-19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             cohort_week             event_week  n_users  retention_rate\n",
       "0  2024-12-30/2025-01-05  2024-12-30/2025-01-05        2        0.666667\n",
       "1  2024-12-30/2025-01-05  2025-01-06/2025-01-12        2        0.500000\n",
       "2  2024-12-30/2025-01-05  2025-01-13/2025-01-19        1        0.000000\n",
       "3  2024-12-30/2025-01-05  2025-01-20/2025-01-26        1        0.000000\n",
       "4  2025-01-06/2025-01-12  2025-01-06/2025-01-12        1        0.500000\n",
       "5  2025-01-06/2025-01-12  2025-01-13/2025-01-19        1        0.000000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(['user', 'event_date'])\n",
    "\n",
    "df['first_date'] = df.groupby('user')['event_date'].transform('min')\n",
    "df['cohort_week'] = df['first_date'].dt.to_period('W')\n",
    "\n",
    "purchases = df.loc[df['event_type'] == 'purchase', ['user', 'event_date']].rename(columns={'event_date': 'purchase_date'})\n",
    "\n",
    "df_combine = df.merge(purchases, on = 'user')\n",
    "\n",
    "df_combine['purchase_next_14d'] = (df_combine['purchase_date'] < df_combine['event_date'] + pd.Timedelta('14D')) & (df_combine['purchase_date'] > df_combine['event_date'])\n",
    "\n",
    "\n",
    "\n",
    "df_combine['event_week'] = df_combine['event_date'].dt.to_period('W')\n",
    "\n",
    "\n",
    "\n",
    "df_combine.groupby(['cohort_week', 'event_week']).agg(n_users = ('user', 'nunique'), retention_rate = ('purchase_next_14d', 'mean')).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
